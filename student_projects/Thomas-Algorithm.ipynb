{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"../math405.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to the Thomas Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In class, we showed how to calculate the solution to linear systems using Gaussian Elimination and LU Decomposition.\n",
    "\n",
    "In the general case we studied we had $ A x = b $ where \n",
    "$$\n",
    "    A = \n",
    "    \\begin{pmatrix}\n",
    "        A_{11} & A_{12} &  \\cdots & A_{1N}  \\\\ \n",
    "        A_{21} & A_{22} &  \\cdots & A_{2N}  \\\\ \n",
    "          \\vdots & \\vdots &        & \\vdots \\\\ \n",
    "        A_{N1} & A_{N2} &  \\cdots & A_{NN}  \\\\ \n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost analysis of LU decomposition\n",
    "\n",
    "In class we showed that a backsubstitution algorthm will be $\\mathcal{O}(N^2)$ so all that is left is to see how expensive our Gaussian elimination will be. For simplicity, assume $+,-,\\times,\\div$ are all the same cost, 1 flop.\n",
    "\n",
    "For each element in the first column $A_{21}$, $A_{31}$, ... , $A_{N1}$ we will need to do $N$ additions (one for each element of the given row). We will need to repeat this for each column, but each time, our starting point in the column and number of additions will each decrease by one.\n",
    "\n",
    "$$\n",
    "    \\sum_{i=1}^{N} \\sum_{j=i}^{N} \\sum_{k=i}^{N+1} \\mathcal{O}(1)\n",
    "    \\implies \\sum_{i=1}^{N} (N - i)(N + 1 - i)\n",
    "$$\n",
    "If we let $j = N - i$, the sum simply reverses and we have \n",
    "$$\n",
    "    \\sum_{j=1}^{N} j(j+1) \\implies \\sum_{j=1}^{N} j^2 + \\sum_{j=1}^{N} j\n",
    "$$\n",
    "These are a known sums, we end up with \n",
    "$$\n",
    "   \\frac{N^3}{3} + \\frac{N^2}{2} + \\frac{N}{6} + \\frac{N(N+1)}{2} \n",
    "$$\n",
    "Which is $\\mathcal{O}(N^3)$, therefore Gaussian elimination dominates the time complexity.\n",
    "\n",
    "**Summary: GE and LU decomposition are great for the general case, but quite slow.**\n",
    "\n",
    "*Question: Are there are some special cases where we can do better?*\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tridiagonal systems\n",
    "Noe that we have shown the cost of using LU decomposition to solve a problem, let's consider a problem where we should be able to do better. \n",
    "\n",
    "Consider the case of a initial value problem with $x \\in (0, 1)$\n",
    "$$\n",
    "     \\dot y = \\frac{\\partial^2 y}{\\partial x^2}\n",
    "$$\n",
    "with initial conditions $y(x, 0) = g(x)$. \n",
    "\n",
    "*Note that I am using the notation* $\\dot y = dy/dt$, $\\ddot y = d^2y/dt^2$\n",
    "\n",
    "Using finite differences, we can show that for our function $y(x, t)$\n",
    "$$\n",
    "    \\frac{\\partial^2 y}{\\partial x^2} \\approx \\frac{y(x + h_x, t) - 2y(x, t) + f(x - h_x, t)}{h_x^2}\n",
    "$$\n",
    "With step size (in $x$) $h_x$ - this can be thought of as nodal spacing. *Please see (1) or (3) for explicit calculations of this.*\n",
    "\n",
    "<!-- Include calculation in comments -->\n",
    "\n",
    "What about in the time domain? Well we can compute a forward difference like so\n",
    "$$\n",
    "    \\dot y \\approx \\frac{y(x, t + h_t) - y(x, t)}{h_t}\n",
    "$$\n",
    "So we get \n",
    "$$\n",
    "    y(x, t + h_t) = y(x, t) + \\frac{h_t}{h_x^2}\\cdot[(y(x + h_x, t) - 2y(x, t) + f(x - h_x, t)]\n",
    "$$\n",
    "Now we have a way to calculate but we will run into issues if the following does not hold.\n",
    "$$\n",
    "    h_t \\leq \\frac{h_x^2}{2} \n",
    "$$\n",
    "*See (3) or (4) for a proof of explicitly why this happens*\n",
    "\n",
    "Despite not going into depth about why this happens, its maybe not so hard to see that there will be a problem coming from the fact that $\\partial^2 y/\\partial x^2$ is not calculated at $t + h_t$ (since we don't know the values that we would need to solve it).\n",
    "\n",
    "We can introduce the Crank-Nicolson scheme and without going into too much detail this will mean that we average on the contribution to the second spatial derivative from timestep $t + h_t$.\n",
    "$$\n",
    "     -\\frac{h_t}{2h_x^2}\\cdot y(x + h_x, t + h_t) + (1 + \\frac{h_t}{h_x^2})\\cdot y(x, t + h_t) -\\frac{h_t}{2h_x^2}\\cdot y(x - h_x, t + h_t) \\\\= -\\frac{h_t}{2h_x^2}\\cdot y(x + h_x, t) + (1 + \\frac{h_t}{h_x^2})\\cdot y(x, t) -\\frac{h_t}{2h_x^2}\\cdot y(x - h_x, t)\n",
    "$$\n",
    "The point of all this is we must solve a tridiagonal linear system like the one below many times (at each timestep) if we want to be able to do better than the initial approach.\n",
    "\n",
    "$$\n",
    "    A = \n",
    "    \\begin{pmatrix}\n",
    "        A_{11} & A_{12} & 0         & 0         & \\cdots     & 0     \\\\ \n",
    "        A_{12} & A_{22} & A_{23}    & 0         & \\cdots     & 0     \\\\ \n",
    "        \\vdots & \\vdots & \\vdots    & \\vdots    & \\vdots             \\\\ \n",
    "        0      & \\cdots & 0         & 0         & A_{N,N-1}  & A_{NN}\\\\ \n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "For the Crank-Nicolson scheme, elements in each row will be, respectively,\n",
    "\n",
    "$$\n",
    "    -\\frac{h_t}{2h_x^2}, (1 + \\frac{h_t}{h_x^2}), -\\frac{h_t}{2h_x^2}\n",
    "$$\n",
    "\n",
    "i.e. the same for the first and last elements.\n",
    "\n",
    "<!-- The off diagonal elements are contributions from neighboring nodes. Note that it is common to use dummy/ghost points to enforce boundary conditions i.e. drop $A_{12}$ and $A_{N,N-1}$ terms. -->\n",
    "\n",
    "But, if we try to use LU decomposition we will end up with $\\mathcal{O}(N^3)$ for each time iteration, surely we can do better than this -- we have so many fewer eliminations to do!\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does the Thomas algorithm actually work?\n",
    "\n",
    "Consider an tridiagonal matrix system\n",
    "\n",
    "$$\n",
    "    A\\mathbf{x} = \\mathbf{d}\\implies\\begin{bmatrix}\n",
    "        b_1 & c_1 & 0 & 0 & \\cdots & 0 & d_1 \\\\\n",
    "        a_1 & b_2 & c_2 & 0 & \\cdots & 0 & d_2 \\\\\n",
    "        0 & a_2 & b_3 & c_3 & \\cdots & 0 & d_3 \\\\\n",
    "        \\vdots & & \\ddots & \\ddots & \\ddots & & \\vdots \\\\\n",
    "        0 & & & a_{n-2} & b_{n-1} & c_{n-1} & d_{n-1} \\\\\n",
    "        0 & & & 0 & a_{n-1} & b_n & d_n\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "First, we divide the first row by $b_1$\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        1 & c_1' & 0 & 0 & \\cdots & 0 & d_1' \\\\\n",
    "        a_1 & b_2 & c_2 & 0 & \\cdots & 0 & d_2 \\\\\n",
    "        0 & a_2 & b_3 & c_3 & \\cdots & 0 & d_3 \\\\\n",
    "        \\vdots & & \\ddots & \\ddots & \\ddots & & \\vdots \\\\\n",
    "        0 & & & a_{n-2} & b_{n-1} & c_{n-1} & d_{n-1} \\\\\n",
    "        0 & & & 0 & a_{n-1} & b_n & d_n\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $c_1' = \\frac{c_1}{b_1}$ and $d_1' = \\frac{d_1}{b_1}$. Then let us multiply the first row by $-a_1$ and add it to the second row\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        1 & c_1' & 0 & 0 & \\cdots & 0 & d_1' \\\\\n",
    "        0 & b_2' & c_2 & 0 & \\cdots & 0 & d_2' \\\\\n",
    "        0 & a_2 & b_3 & c_3 & \\cdots & 0 & d_3 \\\\\n",
    "        \\vdots & & \\ddots & \\ddots & \\ddots & & \\vdots \\\\\n",
    "        0 & & & a_{n-2} & b_{n-1} & c_{n-1} & d_{n-1} \\\\\n",
    "        0 & & & 0 & a_{n-1} & b_n & d_n\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $b_2' = b_2 - a_1c_1'$ and $d_2' = d_2 - a_1d_1'$. Repeating the process, we divide the second row by $b_2'$\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        1 & c_1' & 0 & 0 & \\cdots & 0 & d_1' \\\\\n",
    "        0 & 1 & c_2' & 0 & \\cdots & 0 & d_2'' \\\\\n",
    "        0 & a_2 & b_3 & c_3 & \\cdots & 0 & d_3 \\\\\n",
    "        \\vdots & & \\ddots & \\ddots & \\ddots & & \\vdots \\\\\n",
    "        0 & & & a_{n-2} & b_{n-1} & c_{n-1} & d_{n-1} \\\\\n",
    "        0 & & & 0 & a_{n-1} & b_n & d_n\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        c_2' = \\frac{c_2}{b_2'} = \\frac{c_2}{b_2-a_1c_1'} && d_2'' = \\frac{d_2'}{b_2'} = \\frac{d_2 - a_1d_1'}{b_2 - a_1c_1'}\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "Continuing on, we multiply the second row by $-a_2$ and add it to the third row\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        1 & c_1' & 0 & 0 & \\cdots & 0 & d_1' \\\\\n",
    "        0 & 1 & c_2' & 0 & \\cdots & 0 & d_2'' \\\\\n",
    "        0 & 0 & b_3' & c_3 & \\cdots & 0 & d_3' \\\\\n",
    "        \\vdots & & \\ddots & \\ddots & \\ddots & & \\vdots \\\\\n",
    "        0 & & & a_{n-2} & b_{n-1} & c_{n-1} & d_{n-1} \\\\\n",
    "        0 & & & 0 & a_{n-1} & b_n & d_n\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where $b_3' = b_3 - a_2c_2'$ and $d_3' = d_3 - a_2d_2''$. Again, we divide the third row by $b_3'$\n",
    "\n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        1 & c_1' & 0 & 0 & \\cdots & 0 & d_1' \\\\\n",
    "        0 & 1 & c_2' & 0 & \\cdots & 0 & d_2'' \\\\\n",
    "        0 & 0 & 1 & c_3' & \\cdots & 0 & d_3'' \\\\\n",
    "        \\vdots & & \\ddots & \\ddots & \\ddots & & \\vdots \\\\\n",
    "        0 & & & a_{n-2} & b_{n-1} & c_{n-1} & d_{n-1} \\\\\n",
    "        0 & & & 0 & a_{n-1} & b_n & d_n\n",
    "    \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        c_3' = \\frac{c_3}{b_3'} = \\frac{c_3}{b_3-a_2c_2'} && d_3'' = \\frac{d_3'}{b_3'} = \\frac{d_3 - a_2d_2'}{b_3 - a_2c_2'}\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "Repeating the Gaussian elimination row by row, we end up with a upper triangular matrix (upper bidiagonal matrix)\n",
    "\n",
    "\\[\\begin{bmatrix}\n",
    "    1 & c_1' & 0 & 0 & \\cdots & 0 & d_1' \\\\\n",
    "    0 & 1 & c_2' & 0 & \\cdots & 0 & d_2'' \\\\\n",
    "    0 & 0 & 1 & c_3' & \\cdots & 0 & d_3'' \\\\\n",
    "    \\vdots & & \\ddots & \\ddots & \\ddots & & \\vdots \\\\\n",
    "    0 & & & 0 & 1 & c_{n-1}' & d_{n-1}'' \\\\\n",
    "    0 & & & 0 & 0 & 1 & d_n''\n",
    "\\end{bmatrix}\\]\n",
    "\n",
    "Now, using backward substitution, the solution is\n",
    "\n",
    "$$\n",
    "    \\begin{gather*}\n",
    "        x_n = d_n'' \\\\\n",
    "        x_{n-1} = d_{n-1}'' - c_{n-1}'x_n \\\\\n",
    "        \\vdots \\\\\n",
    "        x_1 = d_1' - c_1'x_2\n",
    "    \\end{gather*}\n",
    "$$\n",
    "Here is the pesudocode for the algorithm. Note that we have the vectors\n",
    "\n",
    "$$\n",
    "    \\begin{align*}\n",
    "        a = \\begin{bmatrix} a_1 & a_2 & \\cdots & a_{n-1} \\end{bmatrix}^T &&\n",
    "        b = \\begin{bmatrix} b_1 & b_2 & \\cdots & b_n \\end{bmatrix}^T \\\\\n",
    "        c = \\begin{bmatrix} c_1 & c_2 & \\cdots & c_{n-1} \\end{bmatrix}^T &&\n",
    "        d = \\begin{bmatrix} d_1 & d_2 & \\cdots & d_n \\end{bmatrix}^T\n",
    "    \\end{align*}\n",
    "$$\n",
    "\n",
    "```\n",
    "# divide the first row by the main diagonal element\n",
    "c[1] = c[1]/b[1]\n",
    "d[1] = d[1]/b[1]\n",
    "\n",
    "# Perform Gaussian elimination: the tridiagonal matrix becomes the upper bidiagonal matrix.\n",
    "for i = 2:n-1\n",
    "    c[i] = c[i]/(b[i] - a[i]*c[i-1])\n",
    "    d[i] = (d[i] - a[i-1]*d[i-1])/(b[i] - a[i-1]*c[i-1])\n",
    "end\n",
    "\n",
    "# Compute d[n]\n",
    "d[n] = (d[n] - a[n-1]*d[n-1])/(b[n] - a[n-1]*c[n-1])\n",
    "\n",
    "# The first solution.\n",
    "x[n] = d[n]\n",
    "\n",
    "# Perform backward substitution, the solution is obtained.\n",
    "for i = n-1:1\n",
    "    x[i] = d[i] - c[i]*x[i+1]\n",
    "end\n",
    "```\n",
    "\n",
    "From the algorithm, we are always dividing by \\(b_n - a_{n-1}c'_n\\) and, hence, division by zero can occur which means numerical stability is not always guaranteed. Hence, we have the sufficient condition: if \\(A\\) is diagonally dominant, the Thomas algorithm never encounters division by zero. A square matrix is diagonally dominant if the absolute value of each diagonal element is greater than the sum of the absolute values of other elements in the row.\n",
    "\\newline\n",
    "\n",
    "A tridiagonal matrix is sparse which means the matrix contains lots of zeroes!  The Thomas algorithm takes advantage and we only need to do simple calculations.\n",
    "Consider the 10 by 10 tridiagonal matrix\n",
    "                                                        \n",
    "$$\n",
    "    \\begin{bmatrix}\n",
    "        b_1 & c_1 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "        a_1 & b_2 & c_2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "        0 & a_2 & b_3 & c_3 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "        0 & 0 & a_3 & b_4 & c_4 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "        0 & 0 & 0 & a_4 & b_5 & c_5 & 0 & 0 & 0 & 0 \\\\\n",
    "        0 & 0 & 0 & 0 & a_5 & b_6 & c_6 & 0 & 0 & 0 \\\\\n",
    "        0 & 0 & 0 & 0 & 0 & a_6 & b_7 & c_7 & 0 & 0 \\\\\n",
    "        0 & 0 & 0 & 0 & 0 & 0 & a_7 & b_8 & c_8 & 0 \\\\\n",
    "        0 & 0 & 0 & 0 & 0 & 0 & 0 & a_8 & b_9 & c_9 \\\\\n",
    "        0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & a_9 & b_{10}\n",
    "    \\end{bmatrix}\n",
    "$$                                                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thomas Algorithm in Action\n",
    "\n",
    "Consider the differential equation we saw in class:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "- u''(x) + r(x) u(x) &= f(x), \\qquad x \\in (0, 1) \\\\ \n",
    "  u(0) &= u_0, \\\\ \n",
    "  u(1) &= u_1\n",
    "\\end{aligned}$$\n",
    "\n",
    "Which we can use the Finite Difference method to approximate as:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    - \\frac{U_{n+1} - 2 U_n + U_{n-1}}{h^2} + R_n U_n  &= F_n, \\quad n = 1, \\dots, N-1, \\\\ \n",
    "    U_0 &= u_0, \\\\ \n",
    "    U_1 &= u_1.\n",
    "\\end{aligned}$$\n",
    "\n",
    "for $h = 1/N$. If we let $r(x) = 1$, $f(x) = 1$, and $u_0 = u_1 = 0$:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "- u''(x) + u(x) &= 1, \\qquad x \\in (0, 1) \\\\ \n",
    "  u(0) &= 0, \\\\ \n",
    "  u(1) &= 0\n",
    "\\end{aligned}$$\n",
    "\n",
    "Then we have the known solution:\n",
    "\n",
    "$$\n",
    "   u(x) = 1 - \\frac{e^x + e^{1-x}}{1+e}\n",
    "$$\n",
    "\n",
    "Where our Finite Difference approximation can be expressed like:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "    1 &  &  &  &        \\\\\n",
    "    - h^{-2} & 2 h^{-2} + 1 & - h^{-2}  &  &      \\\\\n",
    "       & -h^{-2} & 2 h^{-2} + 1 & - h^{-2}  &     \\\\\n",
    "       &    &  \\ddots & \\ddots & \\ddots\n",
    "\\end{pmatrix}\n",
    " \\cdot \n",
    " \\begin{pmatrix}\n",
    "        U_0 \\\\ U_1 \\\\ U_2 \\\\ \\vdots\n",
    " \\end{pmatrix}\n",
    " = \n",
    " \\begin{pmatrix}\n",
    "     0 \\\\ 1 \\\\ 1 \\\\ \\vdots \n",
    " \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Which reduces to the following system after taking out the trivial $U_0$ and $U_N$ rows:\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\n",
    "       2 h^{-2} + 1 & - h^{-2}  &  &      \\\\\n",
    "       -h^{-2} & 2 h^{-2} + 1 & - h^{-2}  &     \\\\\n",
    "          &  \\ddots & \\ddots & \\ddots\n",
    "\\end{pmatrix}\n",
    " \\cdot \n",
    " \\begin{pmatrix}\n",
    "        U_1 \\\\ U_2 \\\\ \\vdots\n",
    " \\end{pmatrix}\n",
    " = \n",
    " \\begin{pmatrix}\n",
    "      1 \\\\ 1 \\\\ \\vdots \n",
    " \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Finally we have a tridiagonal matrix that we can use with the Thomas Algorithm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function thomas(A::Tridiagonal{Float64,Array{Float64,1}}, b::AbstractVector)\n",
    "    N = length(b)\n",
    "    x = zeros(N)\n",
    "    A_ud_ = zeros(N)\n",
    "    b_ = zeros(N)\n",
    "    \n",
    "    # calculate the new upper diagonal values\n",
    "    A_ud_[1] = A[1,2] / A[1,1]\n",
    "    for n = 2:N-1\n",
    "        A_ud_[n] = A[n,n+1] / (A[n,n] - A[n,n-1] * A_ud_[n-1])\n",
    "    end\n",
    "    \n",
    "    # calculate the new b vector\n",
    "    b_[1] = b[1] / A[1,1]\n",
    "    for n = 2:N\n",
    "        b_[n] = (b[n] - A[n,n-1] * b_[n-1]) / (A[n,n] - A[n,n-1] * A_ud_[n-1])\n",
    "    end\n",
    "    # the subdiagonal can now be treated as zero and the diagonal as 1\n",
    "    \n",
    "    # backsubstitution\n",
    "    x[N] = b_[N]\n",
    "    for n = N-1:-1:1\n",
    "        x[n] = b_[n] - A_ud_[n] * x[n+1]\n",
    "    end\n",
    "    \n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for N in (10).^(1:4)\n",
    "    h = 1/N\n",
    "    fn = ones(N-1) # f_0 = 0, f_N = 0\n",
    "\n",
    "    d = ones(N-1) .* (2 / h^2 + 1)\n",
    "    du = ones(N-2) .* (-1 / h^2)\n",
    "    dl = ones(N-2) .* (-1 / h^2)\n",
    "    FD = Tridiagonal(dl, d, du)\n",
    "    FD_fl = Array(FD)\n",
    "    \n",
    "    # make sure all the methods are compiled\n",
    "    @time print(\"\\n\\nN = $N\")\n",
    "    thomas(FD, fn)\n",
    "    FD \\ fn\n",
    "    FD_fl \\ fn\n",
    "    \n",
    "    # compile @time and show \n",
    "    @show isapprox(thomas(FD, fn), FD \\ fn)\n",
    "    @show isapprox(thomas(FD, fn), FD_fl \\ fn)\n",
    "    print(\"Thomas:    \")\n",
    "    @time thomas(FD, fn)\n",
    "    print(\"FD \\\\ fn:   \")\n",
    "    @time FD \\ fn\n",
    "    print(\"FD_fl \\\\ fn:\")\n",
    "    @time FD_fl \\ fn\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u(x) = 1 -  (exp(x) + exp(1-x)) / (1 + MathConstants.e)\n",
    "plot(u, 0, 1, lw=3, label = \"Analytical Solution, u(x)\", size = (800, 400))\n",
    "\n",
    "N = 20\n",
    "h = 1/N\n",
    "xn = 0:h:1\n",
    "fn = ones(N-1)\n",
    "Un = zeros(N+1)\n",
    "\n",
    "d = ones(N-1) .* (2 / h^2 + 1)\n",
    "du = ones(N-2) .* (-1 / h^2)\n",
    "dl = ones(N-2) .* (-1 / h^2)\n",
    "FD = Tridiagonal(dl, d, du)\n",
    "\n",
    "Un[2:N] = thomas(FD, fn)\n",
    "scatter!(xn, Un, ms=5, label = \"Thomas FD, Un\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources\n",
    "\n",
    "1. E. Süli and D. Mayer, An Introduction to Numerical Analysis. Chapters 2 & 3\n",
    "2. https://en.wikipedia.org/wiki/Tridiagonal_matrix_algorithm\n",
    "3. A. Pierce, Math 316: Lecture 8 notes, retrieved from https://www.math.ubc.ca/~peirce/\n",
    "4. https://www.quantstart.com/articles/Tridiagonal-Matrix-Solver-via-Thomas-Algorithm/\n",
    "5. https://en.wikipedia.org/wiki/Crank–Nicolson_method"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Julia 1.5.1",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
