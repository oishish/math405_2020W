{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to the Thomas Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In class, we showed how to calculate the solution to linear systems using Gaussian Elimination and LU Decomposition.\n",
    "\n",
    "In the general case we studied we had $ A x = b $ where \n",
    "$$\n",
    "    A = \n",
    "    \\begin{pmatrix}\n",
    "        A_{11} & A_{12} &  \\cdots & A_{1N}  \\\\ \n",
    "        A_{21} & A_{22} &  \\cdots & A_{2N}  \\\\ \n",
    "          \\vdots & \\vdots &        & \\vdots \\\\ \n",
    "        A_{N1} & A_{N2} &  \\cdots & A_{NN}  \\\\ \n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cost analysis of LU decomposition\n",
    "\n",
    "In class we showed that a backsubstitution algorthm will be $\\mathcal{O}(N^2)$ so all that is left is to see how expensive our Gaussian elimination will be. For simplicity, assume $+,-,\\times,\\div$ are all the same cost, 1 flop.\n",
    "\n",
    "For each element in the first column $A_{21}$, $A_{31}$, ... , $A_{N1}$ we will need to do $N$ additions (one for each element of the given row). We will need to repeat this for each column, but each time, our starting point in the column and number of additions will each decrease by one.\n",
    "\n",
    "$$\n",
    "    \\sum_{i=1}^{N} \\sum_{j=i}^{N} \\sum_{k=i}^{N+1} \\mathcal{O}(1)\n",
    "    \\implies \\sum_{i=1}^{N} (N - i)(N + 1 - i)\n",
    "$$\n",
    "If we let $j = N - i$, the sum simply reverses and we have \n",
    "$$\n",
    "    \\sum_{j=1}^{N} j(j+1) \\implies \\sum_{j=1}^{N} j^2 + \\sum_{j=1}^{N} j\n",
    "$$\n",
    "These are a known sums, we end up with \n",
    "$$\n",
    "   \\frac{N^3}{3} + \\frac{N^2}{2} + \\frac{N}{6} + \\frac{N(N+1)}{2} \n",
    "$$\n",
    "Which is $\\mathcal{O}(N^3)$, therefore Gaussian elimination dominates the time complexity.\n",
    "\n",
    "**Summary: GE and LU decomposition are great for the general case, but quite slow.**\n",
    "\n",
    "*Question: Are there are some special cases where we can do better?*\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tridiagonal systems\n",
    "Noe that we have shown the cost of using LU decomposition to solve a problem, let's consider a problem where we should be able to do better. \n",
    "\n",
    "Consider the case of a initial value problem with $x \\in (0, 1)$\n",
    "$$\n",
    "     \\dot y = \\frac{\\partial^2 y}{\\partial x^2}\n",
    "$$\n",
    "with initial conditions $y(x, 0) = g(x)$. \n",
    "\n",
    "*Note that I am using the notation* $\\dot y = dy/dt$, $\\ddot y = d^2y/dt^2$\n",
    "\n",
    "Using finite differences, we can show that for our function $y(x, t)$\n",
    "$$\n",
    "    \\frac{\\partial^2 y}{\\partial x^2} \\approx \\frac{y(x + h_x, t) - 2y(x, t) + f(x - h_x, t)}{h_x^2}\n",
    "$$\n",
    "With step size (in $x$) $h_x$ - this can be thought of as nodal spacing. *Please see (1) or (3) for explicit calculations of this.*\n",
    "\n",
    "<!-- Include calculation in comments -->\n",
    "\n",
    "What about in the time domain? Well we can compute a forward difference like so\n",
    "$$\n",
    "    \\dot y \\approx \\frac{y(x, t + h_t) - y(x, t)}{h_t}\n",
    "$$\n",
    "So we get \n",
    "$$\n",
    "    y(x, t + h_t) = y(x, t) + \\frac{h_t}{h_x^2}\\cdot[(y(x + h_x, t) - 2y(x, t) + f(x - h_x, t)]\n",
    "$$\n",
    "Now we have a way to calculate but we will run into issues if the following does not hold.\n",
    "$$\n",
    "    h_t \\leq \\frac{h_x^2}{2} \n",
    "$$\n",
    "*See (3) or (4) for a proof of explicitly why this happens*\n",
    "\n",
    "Despite not going into depth about why this happens, its maybe not so hard to see that there will be a problem coming from the fact that $\\partial^2 y/\\partial x^2$ is not calculated at $t + h_t$ (since we don't know the values that we would need to solve it).\n",
    "\n",
    "We can introduce the Crank-Nicolson scheme and without going into too much detail this will mean that we average on the contribution to the second spatial derivative from timestep $t + h_t$.\n",
    "$$\n",
    "     -\\frac{h_t}{2h_x^2}\\cdot y(x + h_x, t + h_t) + (1 + \\frac{h_t}{h_x^2})\\cdot y(x, t + h_t) -\\frac{h_t}{2h_x^2}\\cdot y(x - h_x, t + h_t) \\\\= -\\frac{h_t}{2h_x^2}\\cdot y(x + h_x, t) + (1 + \\frac{h_t}{h_x^2})\\cdot y(x, t) -\\frac{h_t}{2h_x^2}\\cdot y(x - h_x, t)\n",
    "$$\n",
    "The point of all this is we must solve a tridiagonal linear system like the one below many times (at each timestep) if we want to be able to do better than the initial approach.\n",
    "\n",
    "$$\n",
    "    A = \n",
    "    \\begin{pmatrix}\n",
    "        A_{11} & A_{12} & 0         & 0         & \\cdots     & 0     \\\\ \n",
    "        A_{12} & A_{22} & A_{23}    & 0         & \\cdots     & 0     \\\\ \n",
    "        \\vdots & \\vdots & \\vdots    & \\vdots    & \\vdots             \\\\ \n",
    "        0      & \\cdots & 0         & 0         & A_{N,N-1}  & A_{NN}\\\\ \n",
    "    \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "For the Crank-Nicolson scheme, elements in each row will be, respectively,\n",
    "\n",
    "$$\n",
    "    -\\frac{h_t}{2h_x^2}, (1 + \\frac{h_t}{h_x^2}), -\\frac{h_t}{2h_x^2}\n",
    "$$\n",
    "\n",
    "i.e. the same for the first and last elements.\n",
    "\n",
    "<!-- The off diagonal elements are contributions from neighboring nodes. Note that it is common to use dummy/ghost points to enforce boundary conditions i.e. drop $A_{12}$ and $A_{N,N-1}$ terms. -->\n",
    "\n",
    "But, if we try to use LU decomposition we will end up with $\\mathcal{O}(N^3)$ for each time iteration, surely we can do better than this -- we have so many fewer eliminations to do!\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources\n",
    "\n",
    "1. E. Süli and D. Mayer, An Introduction to Numerical Analysis. Chapters 2 & 3\n",
    "2. https://en.wikipedia.org/wiki/Tridiagonal_matrix_algorithm\n",
    "3. A. Pierce, Math 316: Lecture 8 notes, retrieved from https://www.math.ubc.ca/~peirce/\n",
    "4. https://www.quantstart.com/articles/Tridiagonal-Matrix-Solver-via-Thomas-Algorithm/\n",
    "5. https://en.wikipedia.org/wiki/Crank–Nicolson_method"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Julia 1.5.1",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
