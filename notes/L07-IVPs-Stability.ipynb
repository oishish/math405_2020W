{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# MATH 405/607 \n",
    "\n",
    "# Numerical Methods for Differential Equations\n",
    "\n",
    "[[Instructor: Christoph Ortner]](http://www.math.ubc.ca/~ortner/)  [[CANVAS]](https://canvas.ubc.ca/courses/55324)\n",
    "\n",
    "\n",
    "## IVPs Part 2: Stability\n",
    "\n",
    "* Stiff problems\n",
    "* Dissipative behaviour \n",
    "* Implicit Euler and Crank-Nicholson\n",
    "* Outlook: Implicit Runge-Kutta Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "include(\"../math405.jl\")\n",
    "\n",
    "function euler(f, u0, h, T)\n",
    "    t = 0.0:h:T \n",
    "    U = zeros(length(u0), length(t)); U[:, 1] = u0 \n",
    "    for n = 2:length(t) \n",
    "        U[:,n] = U[:,n-1] + h * f(t[n-1], U[:,n-1])\n",
    "    end \n",
    "    return U, t\n",
    "end\n",
    "\n",
    "function improved_euler(f, u0, h, T)\n",
    "    t = 0.0:h:T \n",
    "    U = zeros(length(u0), length(t)); U[:, 1] = u0 \n",
    "    for n = 2:length(t) \n",
    "        F = f(t[n-1], U[:,n-1])\n",
    "        U[:,n] = U[:,n-1] + 0.5 * h * (F + f(t[n], U[:,n-1] + h * F))\n",
    "    end \n",
    "    return U, t\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Literature\n",
    "\n",
    "* D. Mayers and E. Suli, An Introduction to Numerical Analysis, CUP; ยง12.4, 12.7, 12.11, 12.12\n",
    "* N. Trefethen, https://people.maths.ox.ac.uk/trefethen/1all.pdf, ยง1.8 \n",
    "* I also quite liked [these lecture notes](https://webspace.science.uu.nl/~frank011/Classes/numwisk/ch10.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A completely artificial but illuminating example [[Tref, Ex.1.8.1]](https://people.maths.ox.ac.uk/trefethen/1all.pdf):\n",
    "$$\n",
    "    \\dot{u}(t) = - 100(u(t) - \\cos(t)) - \\sin(t), \\qquad u(0) = 1.\n",
    "$$\n",
    "\n",
    "The (unique) solution is obviously $u(t) = \\cos(t)$, which is smooth (all derivatives are bounded by 1) and e.g. the truncation errors of the Euler and Improved Euler methods will be $|T_n| \\leq h, |T_n| \\leq h^2$. So we can probably afford to take quite moderate steps..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "f = (t, u) -> [ - 100 * (u[1] - cos(t)) - sin(t) ]\n",
    "h = 0.03; u0 = [1.0]; Tf = 1.0\n",
    "Ue, te = euler(f, u0, h, Tf)\n",
    "Ui, ti = improved_euler(f, u0, h, Tf)\n",
    "plot(cos, 0, Tf, lw=3, label = \"exact\", size = (500, 300), yaxis = ([-3.0, 3.0],))\n",
    "plot!(te, Ue[1,:], lw=3, label = \"Euler\")\n",
    "plt_003 = plot!(ti, Ui[1,:], lw=3, label = \"Improved Euler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# change the time-step from 0.03 to 0.02 !!!!\n",
    "h = 0.02;\n",
    "Ue, te = euler(f, u0, h, Tf)\n",
    "Ui, ti = improved_euler(f, u0, h, Tf)\n",
    "plot(cos, 0, Tf, lw=3, label = \"exact\", size = (500, 300), yaxis = ([-3.0, 3.0],))\n",
    "plot!(te, Ue[1,:], lw=3, label = \"Euler\")\n",
    "plt_002 = plot!(ti, Ui[1,:], lw=3, label = \"Improved Euler\")\n",
    "plot(plt_003, plt_002, size = (600, 300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what is going on????? Is this not a contradiction to our error analysis? (QUESTION: what have I missed?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To get an idea of what is going on we look not just at one trajectory but *all* trajectories of the ODE: We observe that trajectories \"nearby\" the solution $u(t) = \\cos(t)$ are technically smooth but vary rapidly $(\\dot{u} = O(100))$, and small perturbations from the solution (such as a numerical error!) will move our numerical solution onto a different trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.003  # to be on the safe side...\n",
    "plt = plot(cos, 0, Tf, c=2, yaxis = ([0.5, 1.5],), xaxis = ([-0.01, 0.15],), lw=2, label = \"different u0\", size = (500, 300))\n",
    "for u0 = ( [1.1], [1.5], [10.0], [100.0], [2000.0], \n",
    "            [0.9], [0.5], [-10.0], [-100.0], [-2000.0])\n",
    "    Ui, ti = improved_euler(f, u0, h, Tf)\n",
    "    plot!(ti, Ui[1,:], lw=2, c=2, label = \"\")\n",
    "end\n",
    "plot!(cos, 0, Tf, lw=3, c=1, label = \"u0 = 1\")\n",
    "plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The general take-away message is that in analysing numerical schemes it is often crucial to understand how the underlying problem changes under small perturbations. The case we are considering here is \n",
    "\n",
    "**Loose Definition:** A problem is called *stiff* if the exact solution is smooth / varies slowly, but there are nearby solutions that vary rapidly. In other words the family of solutions varies on different time-scales.\n",
    "\n",
    "The **very load hint** in the example  above is  the factor 100: \n",
    "$$\n",
    "    \\dot{u} = - 100 ( u - \\cos(t) ) - \\sin(t)\n",
    "$$\n",
    "\n",
    "For those interested in dynamical systems: a common occurance is when there is a smooth attractor with nearby trajectories converging rapidly towards the attractor. This is precisely the example shown above. But there are other cases.\n",
    "\n",
    "I highly recommend reading Nick Trefethen's discussion of stiff problems in [Sec. 1.7 of his lecture notes](https://people.maths.ox.ac.uk/trefethen/1all.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stability Regions\n",
    "\n",
    "The simplest possible setting where we can think of is a homogeneous equation \n",
    "$$\n",
    "    \\dot{u} = f(u)\n",
    "$$\n",
    "with equilibrium $f(u_*) = 0$ and $f : \\mathbb{R} \\to \\mathbb{R}$ (i.e. a scalar ODE). Then $u(t) = u_*$ is obviously a solution. \n",
    "\n",
    "We assume moreover this is a stable equilibrium i.e. all nearby trajectories converge to $u_*$. This can be characterised by $f'(u_*) < 0$. (The case $f'(u_*) = 0$ is a bit trickier and we will ignore this!) (I will record a lecture to review all this material!)\n",
    "\n",
    "Formally, we can linearlise the ODE around $u_*$. Let $v(t) = u(t) - u_*$ then we obtain \n",
    "$$ \n",
    "    \\dot{v}(t) = f'(u_*) v(t) + O(v^2)\n",
    "$$\n",
    "We now drop the $O(v^2)$ term, rename $u = v$ and obtain our model problem \n",
    "$$ \n",
    "    \\dot{u}(t) = \\lambda u(t)\n",
    "$$\n",
    "where $\\lambda < 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In fact it is customary to consider $\\lambda \\in \\mathbb{C}, {\\rm Re} \\lambda < 0$;\n",
    "$$\n",
    "    \\dot{u}(t) = \\lambda u(t)\n",
    "$$\n",
    "Then $u(t) = e^{\\lambda t} u(0) \\to 0$ with rate $e^{{\\rm Re}\\lambda t}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**The challenge:** under which conditions does the numerical solution also converge to zero? \n",
    "\n",
    "Aside from giving us an intuition about how the method will behave for stiff problems this is also interesting in the context of qualitative long-term behaviour of solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example:** Euler Method \n",
    "\n",
    "$$\n",
    "U_{n+1} = U_n + h \\lambda U_n = (1+h\\lambda) U_n.\n",
    "$$ \n",
    "\n",
    "Note how $h\\lambda$ appears together; this is *generic* and we will therefore write $z = h \\lambda$ which gives \n",
    "\n",
    "$$\n",
    "U_{n+1} = (1+z) U_n.\n",
    "$$\n",
    "\n",
    "We have that $|U_n| \\to 0$ iff. $|1+z| < 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Plots.pyplot() # (the following function doesn't work with the GR backend...)\n",
    "stab_euler = MATH405.levelset([-3, 1], [-2,2], (x,y)->((x+1)^2 + y^2), 1.0, title = \"Stability Euler Method\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Same analysis for the Improved Euler Method \n",
    "$$\n",
    "    U_{n+1} = U_n + \\frac{h}{2} \\Big( \\alpha U_n + \\alpha (U_n + h \\alpha U_n) \\Big) = \n",
    "            \\Big(1 + z + \\frac{z^2}{2} \\Big) U_n\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mult = z -> 1 + z + z^2/2\n",
    "stab_ie = MATH405.levelset( [-3,1], [-2,2], (x, y) -> abs( mult(x + im * y) ), 1.0, c=2, title = \"Stability Improved Euler\")\n",
    "plot( stab_euler, stab_ie, size = (600, 300) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Anlogue of our toy problem: \n",
    "$$\n",
    "\\dot{u} = - 100 u \n",
    "$$ \n",
    "$|1+z| < 1$, $|-100 h + 1| < 1$, \n",
    "$$\n",
    "    100^2 h^2 + 200 h + 1 < 1 \n",
    "$$\n",
    "$$\n",
    "     h < 2 /100 = 0.02\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Remark on Systems\n",
    "\n",
    "If $f : \\mathbb{R}^N \\to \\mathbb{R}^N$ with $f(u_*) = 0$ and we linearise around $u_*$ then we get a linear ODE system \n",
    "$$ \n",
    "    \\dot{v}(t) = A v(t),\n",
    "$$\n",
    "where $A = \\partial f(u_*)$. \n",
    "\n",
    "* The stability can then be understood in terms of the spectrum $\\sigma(A)$ : if all eigenvalues have negative real part then $v(t) \\to 0$ exponentially fast. If $A$ is diagonalisable, then we can transform the system to $\\dot{w}_i(t) = \\lambda_i w_i(t)$, $i = 1, \\dots, N$ and each of these equations is one of the $\\dot{u}(t) = \\lambda u(t)$ equations we analyzed above. (This is why we assumed $\\lambda \\in \\mathbb{C}$.) Specifically, we now require that *all* scaled eigenvalues $h\\lambda_i$ belong to the stability region of the numerical method! I will go into this in more detail in a recorded lecture.\n",
    "* The stiffness of $\\dot{u} = f(u)$ near $u_*$ can also be understood similarly; if $\\sigma(A)$ containes eigenvalues of significantly  different magnitude then this indicates different modes of the system evolving at different speeds, which is a strong indicator of stiffness. See also [[Tref, Sec. 1.8]](https://people.maths.ox.ac.uk/trefethen/1all.pdf) for a very nice discussion of this connection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stability Regions for RK Methods \n",
    "\n",
    "\n",
    "* Every (explicit) Runge-Kutta method applied to $\\dot{u} = \\lambda u$ takes the form \n",
    "\n",
    "$$ \n",
    "    U_{n+1} = p(\\lambda h) U_n\n",
    "$$\n",
    "\n",
    "where $p$ is a polynomial. \n",
    "\n",
    "* The *stability region* is therefore given by the sublevel set \n",
    "\n",
    "$$ \n",
    "    \\{ z : |p(z)| < 1 \\}.\n",
    "$$\n",
    "\n",
    "* Because $|p(z)| \\to \\infty$ as $|z| \\to \\infty$ it follows that RK methods all have compact stability regions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# E.g. for the RK4 method (exercise!)\n",
    "mult_e   = z -> 1+z\n",
    "mult_ie  = z -> 1 + z + z^2/2\n",
    "mult_rk4 = z -> 1 + z + z^2/2 + z^3/6 + z^4/24 \n",
    "plt = MATH405.levelset([-3.5,1.5], [-3.5, 3.5], [ (x, y) -> abs(mult(x + im * y)) \n",
    "                                                  for mult in [mult_rk4, mult_ie, mult_e] ],\n",
    "                       1.0, xy=true, fill=true, label = (\"RK4\", \"Improved Euler\", \"Euler\"), \n",
    "                       legend = :outertopright, size = (380, 300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Challenge: construct RK methods with \n",
    "* maximal stability regions?\n",
    "* maximise range of stability along the negative real axis? \n",
    "\n",
    "This is a fun game; cf RKC, ROCK, DUMKA methods; see e.g. \n",
    "* [1] https://link.springer.com/article/10.1007%2Fs002110100292\n",
    "* [2] https://epubs.siam.org/doi/abs/10.1137/S1064827500379549\n",
    "\n",
    "This gives you methods with very interesting stability regions: \n",
    "\n",
    "<img href=\"figs/stabrock2.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\dot{u} = \\lambda u\n",
    "$$\n",
    "### A-Stability\n",
    "\n",
    "* But wouldn't it be nice to have numerical integrators for which the entire left half-place $\\{ z : {\\rm Re} z < 0\\}$ is in the stability region?\n",
    "* We know from the previous slides that this cannot be an (explicit) Runge-Kutta method. What else can we try?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Remember that in the derivation of the IE method we had an intermediate result, the Crank-Nicholson scheme: \n",
    "\n",
    "$$ \n",
    "  U_{n+1} = U_n + \\frac{h}{2} \\big( f(t_n, U_n) + f(t_{n+1}, U_{n+1}) \\big).\n",
    "$$\n",
    "\n",
    "Because each time-step required the solution of a nonlinear system, we call this an *implicit* integrator. The simplest example of its kind is the *Implicit Euler Method:*\n",
    "\n",
    "$$\n",
    "    U_{n+1} = U_n + h f(t_{n+1}, U_{n+1})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us apply the implicit Euler method to our $\\dot{u} = \\lambda u$ model problem ... \n",
    "\n",
    "$$ \\begin{aligned} \n",
    "    & U_{n+1} = U_n + h \\lambda U_{n+1} = U_n + z U_{n+1} \\\\ \n",
    "    \\Leftrightarrow \\qquad & (1-z) U_{n+1} = U_n  \\\\ \n",
    "    \\Leftrightarrow \\qquad & U_{n+1} = (1 - z)^{-1} U_n\n",
    "\\end{aligned}$$\n",
    "I.e. the region of stability = $\\{ |1-z|^{-1} < 1 \\}$, the exterior of the disk $\\{ |1-z| \\leq 1\\}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "r_impe = z -> 1/(abs(1-z)+1e-15)\n",
    "MATH405.levelset([-2.5,2.5], [-2.5,2.5], (x, y) -> abs( r_impe(x + im * y) ), 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Definition:** We call a numerical integrator *A-stable* if its stability region includes the entire left half plane $\\{ z : {\\rm Im} z < 0 \\}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Crank-Nicholson method:** exercise "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Outlook: Implicit Runge-Kutta Methods\n",
    "\n",
    "Recall the *explicit* RK methods: \n",
    "$$\\begin{aligned}\n",
    "    k_1 &= h f(t_n, U_n) \\\\ \n",
    "    k_2 &= h f(t_n + c_2 h, U_n + a_{21} k_1) \\\\ \n",
    "    k_3 &= h f(t_n + c_3 h, U_n + a_{31} k_1 + a_{32} k_2) \\\\ \n",
    "    &\\vdots \\\\ \n",
    "    k_{s} &= h f(t_n + c_{s} h, U_n + a_{s,1} k_1 + \\dots + a_{s,s-1} k_{s-1}) \\\\ \n",
    "    U_{n+1} &= U_n + b_1 k_1 + b_2 k_2 + \\dots + b_s k_s\n",
    "\\end{aligned}$$\n",
    "\n",
    "described by *lower-triangular* Butcher tables \n",
    "$$\n",
    "\\begin{array}{c|ccccc}\n",
    "    0      &       & & & &  \\\\\n",
    "    c_2    & a_{21}   &&& &   \\\\\n",
    "    c_3    & a_{31} & a_{32} & & &   \\\\\n",
    "    \\vdots &   \\vdots     &   & \\ddots & &  \\\\\n",
    "    c_{s}    & a_{s,1} & a_{s,2} & \\cdots & a_{s,s-1} &   \\\\ \\hline\n",
    "           &  b_1 & b_2 & \\cdots & b_{s-1} & b_s \n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "By contrast, *implicit* RK formulas take the form \n",
    "$$\\begin{aligned}\n",
    "    k_j &= h f(t_n + c_j h, U_n + a_{j1} k_1 + \\dots + a_{js} k_s) \\\\ \n",
    "    U_{n+1} &= U_n + b_1 k_1 + b_2 k_2 + \\dots + b_s k_s\n",
    "\\end{aligned}$$\n",
    "\n",
    "described by *full* Butcher tables \n",
    "$$\n",
    "\\begin{array}{c|ccccc}\n",
    "    c_1    & a_{11}  & a_{12} & \\cdots & a_{1,s}   \\\\\n",
    "    c_2    & a_{21}  & a_{22} & \\cdots & a_{2,s} \\\\\n",
    "    \\vdots & \\vdots  &        &        & \\vdots  \\\\\n",
    "    c_{s}  & a_{s,1} & a_{s,2}& \\cdots & a_{s,s} \\\\ \\hline\n",
    "           &  b_1 & b_2 & \\cdots & b_s \n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The theory is much cleaner ... \n",
    "\n",
    "![trefirk](figs/trefethen_irk_thm.png)\n",
    "\n",
    "so why aren't we using IRK methods all the time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Main Caveat:** One step of an IRK method requires the solution of a nonlinear system for $k_1, \\dots, k_s$.\n",
    "    \n",
    "But many compromises and tricks have been devised to alleviate this additional cost, and we will encounter some of them later on in the course. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For example, what if the ODE is of the form \n",
    "$$ \n",
    "    \\dot{u} = Au + g(u) \n",
    "$$\n",
    "where the term $Au$ is stiff but $g(u)$ is not, then we could discretise by \n",
    "$$ \n",
    "    U_{n+1} = U_n + h \\big( A U_{n+1}  + g(u_n) \\big)\n",
    "$$\n",
    "\n",
    "Schemes of this kind are called *semi-implicit*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Or maybe we could devise RK methods with lower-triangular tables, \n",
    "$$\n",
    "\\begin{array}{c|ccccc}\n",
    "    c_1    & a_{11}  &  &  &    \\\\\n",
    "    c_2    & a_{21}  & a_{22} &  &  \\\\\n",
    "    \\vdots & \\vdots  &        &  \\ddots      &   \\\\\n",
    "    c_{s}  & a_{s,1} & a_{s,2}& \\cdots & a_{s,s} \\\\ \\hline\n",
    "           &  b_1 & b_2 & \\cdots & b_s \n",
    "\\end{array}\n",
    "$$\n",
    "Then we would solve a sequence of smaller nonlinear problems instead of one big problem.\n",
    "\n",
    "These are called *Diagonally Implicit Runge-Kutta (DIRK) formulae*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Experiments\n",
    "\n",
    "Implementing an implicit scheme requires some extra work, we will do this in the next workshop. Instead we will first revisit \n",
    "our opening example and then explore the Julia `OrdinaryDiffEq.jl` suite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\n",
    "    \\dot{u}(t) = - 100(u(t) - \\cos(t)) - \\sin(t), \\qquad u(0) = 1.\n",
    "$$\n",
    "What makes our life easy here is that $f$ is scalar and linear in $u$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "function cn_example(u0, h, Tf)\n",
    "    f = (t, u) -> - 100 * (u - cos(t)) - sin(t) \n",
    "    df = - 100\n",
    "    t = 0.0:h:Tf \n",
    "    U = zeros(1, length(t)); U[1, 1] = u0[1]\n",
    "    for n = 2:length(t) \n",
    "        f0 = f(t[n-1], U[1,n-1])\n",
    "        f1 = f(t[n], U[1,n-1])\n",
    "        dU = (h/2 * f0 + h/2 * f1) / (1 - h/2 * df)\n",
    "        U[1, n] = U[1,n-1] + dU\n",
    "    end \n",
    "    return U, t\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "h = 0.03; u0 = [1.0]; Tf = 1.0\n",
    "Ui, ti = improved_euler(f, u0, h, Tf)\n",
    "Ue, te = euler(f, u0, h, Tf)\n",
    "U_cn, t_cn = cn_example(u0, h, Tf)\n",
    "plot(cos, 0, Tf, lw=3, label = \"exact\", size = (500, 300), yaxis = ([-3.0, 3.0],))\n",
    "plot!(te, Ue[1,:], lw=3, label = \"Euler\")\n",
    "plot!(ti, Ui[1,:], lw=3, label = \"Improved Euler\")\n",
    "plot!(t_cn, U_cn[1,:], lw=3, label = \"Crank-Nicholson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# perfect long-time behaviour even with large time-steps!\n",
    "h = 0.1; u0 = [1.0]; Tf = 10.0\n",
    "U_cn, t_cn = cn_example(u0, h, Tf)\n",
    "plot(cos, 0, Tf, lw=3, label = \"exact\", size = (500, 300), yaxis = ([-3.0, 3.0],))\n",
    "plot!(t_cn, U_cn[1,:], lw=3, label = \"Crank-Nicholson\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's try some more advanced solvers from [DifferentialEquations.jl](https://diffeq.sciml.ai/dev/solvers/ode_solve/). Amongst many others, they recommend `Tsit5()` for non-stiff problems and `Rodas5()` for stiff problems. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "using OrdinaryDiffEq\n",
    "f = (u, p, t) -> - 100 * (u - cos(t)) - sin(t) \n",
    "prob = ODEProblem(f, 1.0, (0.0, 10.0))\n",
    "\n",
    "println(\"TSIT5 Integrator (non-stiff)\")\n",
    "@time sol_tsit5 = solve(prob, Tsit5(), abstol=1e-3)\n",
    "@show length(sol_tsit5.t);\n",
    "\n",
    "println(\"RODAS5 Integrator (stiff)\")\n",
    "@time sol_rodas5 = solve(prob, Rodas5(), abstol=1e-3)\n",
    "@show length(sol_rodas5.t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pushing the limits...  \n",
    "\n",
    "The [ROBER test problem](https://www.unige.ch/~hairer/testset/testset.html); H.H. Robertson in 1966 [Rob66]; see also [this nice summary](https://www.radford.edu/~thompson/vodef90web/problems/demosnodislin/Single/DemoRobertson/demorobertson.pdf)\n",
    "\n",
    "$$\\begin{aligned}\n",
    "    \\dot{u}_1 &= - 0.04 u_1 + 10^4 u_2 u_3 \\\\ \n",
    "    \\dot{u}_2 &= 0.04 u_1 - 3\\times 10^7 u_2^2 - 10^4 u_2 u_3 \\\\ \n",
    "    \\dot{u}_3 &= 3 \\times 10^7 u_2^2\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# this implementation rescales u[2] -> sqrt(3e7) u[2]; only to simplify plotting...\n",
    "rober = let kโ = 0.04, kโ = 3e7, kโ = 1e4\n",
    "    sk2 = sqrt(kโ)\n",
    "    (u, p, t) -> SA[      -kโ*u[1] + (kโ/sk2)*u[2]*u[3], \n",
    "                     sk2*( kโ*u[1] - (kโ/sk2)*u[2]*u[3] - u[2]^2 ), \n",
    "                                                          u[2]^2 ]\n",
    "end \n",
    "\n",
    "prob = ODEProblem(rober, [1.0, 0.0, 0.0], (0.0, 1e5))\n",
    "@time rober_tsit5 = solve(prob, Tsit5(); maxiters=10_000); @show length(rober_tsit5.t)\n",
    "@time rober_rodas5 = solve(prob, Rodas5());  @show length(rober_rodas5.t)\n",
    "plot(rober_rodas5, tspan=(1e-6,1e5), xscale = :log10, lw=2, yaxis = [-0.2, 1.2], size=(500, 250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot( plot(rober_tsit5, tspan=(1e-6, rober_tsit5.t[end]), xscale = :log10, lw=1, yaxis = [-0.2, 1.2]),\n",
    "    plot(rober_tsit5, tspan=(1.0, rober_tsit5.t[end]), xscale = :log10, lw=1, yaxis = [0.1115, 0.1125],  xaxis = [5.27, 5.31]), \n",
    "      size=(600, 300))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary\n",
    "\n",
    "* stability regions give restrictions on time-steps, dependent on local **variations** in the driving force $f$\n",
    "* For a more general and comprehensive theory we need to look at perturbation theory for ODEs and diagonalisation of the linearised equations but this leads essentially to the same characterisatons of stability that we have obtained in this lecture\n",
    "* Modern ODE solvers (integrators) are finely tuned and sophisticated codes that can solve IVPs very efficiently; there are multiple classes of integrators, here we distinguished primarily explicit and implicit solvers which are more or less useful for non-stiff / stiff systems.\n",
    "* You should not normally write your own ODE solvers but leverage existing software! But understanding the theory behind the different methods will help you choose the right integrators.\n",
    "* A key missing ingredient in our theory is adaptivity - see the workshops!\n",
    "\n",
    "#### Why does all this not contradict our theory from last week? \n",
    "i\n",
    "Best to think that the \"naive\" theory from last week is only useful when you have moderate Lipschitz constants. In the stiff case, a much finer analysis is required to understand why ODE solvers are accurate. Today we explained this only *qualitatively* but a complete and rigorous analysis is a bit more subtle and goes beyond  the scope of this course."
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 1.5.1",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
